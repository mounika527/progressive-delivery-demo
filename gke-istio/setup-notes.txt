#!/bin/sh

## Configure your project
#
#gcloud init

## Set the default project
#
export GCP_PROJECT=customer-success-242311

#gcloud config set project ${GCP_PROJECT}

## Set the default compute region and zone
#
#gcloud config set compute/region us-east4
#gcloud config set compute/zone us-east4-c
export GCLOUD_REGION=us-east4
export GCLOUD_ZONE=us-east4-c

## Enable the Kubernetes and Cloud DNS services for your project
#
#gcloud services enable container.googleapis.com
#gcloud services enable dns.googleapis.com

## Install kubectl
#
#gcloud components install kubectl

export K8S_VERSION=$(gcloud container get-server-config --format=json  | jq -r '.validMasterVersions[0]')

#export CLUSTER_NAME="istio"
export CLUSTER_NAME="shared-flagger-demo"

gcloud beta container clusters create shared-flagger-demo --cluster-version=${K8S_VERSION} --zone=us-east4-c --num-nodes=3 --machine-type=n1-highcpu-4 --preemptible --no-enable-cloud-logging --no-enable-cloud-monitoring --disk-size=30 --enable-autorepair --addons=HorizontalPodAutoscaling,Istio --istio-config=auth=MTLS_PERMISSIVE

## NOTE!
#
# The above command will create a default node pool consisting of two n1-highcpu-4 
# (vCPU: 4, RAM 3.60GB, DISK: 30GB) preemptible VMs. 
# Preemptible VMs are up to 80% cheaper than regular instances and are terminated and 
# replaced after a maximum of 24 hours.


## Set up credentials for kubectl:
#
gcloud container clusters get-credentials ${CLUSTER_NAME}

## Create a cluster admin role binding:
#
kubectl create clusterrolebinding "cluster-admin-$(whoami)" --clusterrole=cluster-admin --user="$(gcloud config get-value core/account)"

# NOTE - The GKE user requires Kubernetes Engine Admin or Owner role in the GKE project in order to create
#        clusterrolebinding resources, or they will get this error.
#
#!#!# Error from server (Forbidden): clusterrolebindings.rbac.authorization.k8s.io is forbidden: User "sebastian@weave.works" cannot create resource "clusterrolebindings" in API group "rbac.authorization.k8s.io" at the cluster scope: Required "container.clusterRoleBindings.create" permission.

## Validate your setup with:
#
kubectl -n istio-system get svc

## In a couple of seconds GCP should allocate an external IP to the istio-ingressgateway service.

### Create a managed zone named istio in Cloud DNS (replace example.com with your domain):
#
export DOMAIN="sebastianbernheim.com"

gcloud dns managed-zones create --dns-name="${DOMAIN}." --description="Istio zone" "istio"

## Look up your zone's name servers:
#
gcloud dns managed-zones describe istio

## Update your registrar's name server records with the records returned by the above command.

## Wait for the name servers to change (replace example.com with your domain):
#
watch dig +short NS ${DOMAIN}

## Create a static IP address named istio-gateway using the Istio ingress IP:
export GATEWAY_IP=$(kubectl -n istio-system get svc/istio-ingressgateway -ojson | jq -r .status.loadBalancer.ingress[0].ip)

gcloud compute addresses create istio-gateway --addresses ${GATEWAY_IP} --region ${GCLOUD_REGION}

## Create the following DNS records (replace example.com with your domain):

gcloud dns record-sets transaction start --zone=istio

gcloud dns record-sets transaction add --zone=istio --name="${DOMAIN}" --ttl=120 --type=A ${GATEWAY_IP}

gcloud dns record-sets transaction add --zone=istio --name="www.${DOMAIN}" --ttl=120 --type=A ${GATEWAY_IP}

gcloud dns record-sets transaction add --zone=istio --name="*.${DOMAIN}" --ttl=120 --type=A ${GATEWAY_IP}
gcloud dns record-sets transaction execute --zone istio

## Verify that the wildcard DNS is working (replace example.com with your domain):
watch host "test.${DOMAIN}"

## Install the Helm command-line tool:
#
#brew install kubernetes-helm

## Create a service account and a cluster role binding for Tiller:
#
kubectl -n kube-system create sa tiller

kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller

# NOTE - The GKE user requires Kubernetes Engine Admin or Owner role in the GKE project in order to create
#        clusterrolebinding resources, or they will get this error.
#
#!#!# Error from server (Forbidden): clusterrolebindings.rbac.authorization.k8s.io is forbidden: User "sebastian@weave.works" cannot create resource "clusterrolebindings" in API group "rbac.authorization.k8s.io" at the cluster scope: Required "container.clusterRoleBindings.create" permission.

## Deploy Tiller in the kube-system namespace:
#
helm init --service-account tiller

## NOTE!
#
# You should consider using SSL between Helm and Tiller, for more information on securing 
# your Helm installation see docs.helm.sh.

## Install cert-manager's CRDs:
#
export CERT_REPO=https://raw.githubusercontent.com/jetstack/cert-manager

kubectl apply -f ${CERT_REPO}/release-0.7/deploy/manifests/00-crds.yaml

## Create the cert-manager namespace and disable resource validation:
#
kubectl create namespace cert-manager

kubectl label namespace cert-manager certmanager.k8s.io/disable-validation=true

## Install cert-manager with Helm:
#
helm repo add jetstack https://charts.jetstack.io && \
helm repo update && \
helm upgrade -i cert-manager --namespace cert-manager --version v0.7.0 jetstack/cert-manager

#!#!# Error: UPGRADE FAILED: configmaps is forbidden: User "system:serviceaccount:kube-system:tiller" cannot list resource "configmaps" in API group "" in the namespace "kube-system"

# Set FLAGGER_REPO env var
#
export FLAGGER_REPO=https://raw.githubusercontent.com/weaveworks/flagger/master

# Create a generic Istio Gateway to expose services outside the mesh on HTTPS:
#
kubectl apply -f ${FLAGGER_REPO}/artifacts/gke/istio-gateway.yaml

# Create a service account with Cloud DNS admin role
#
#export GCP_PROJECT=customer-success-242311

gcloud iam service-accounts create dns-admin --display-name=dns-admin --project=${GCP_PROJECT}

gcloud iam service-accounts keys create ./gcp-dns-admin.json --iam-account=dns-admin@${GCP_PROJECT}.iam.gserviceaccount.com --project=${GCP_PROJECT}

gcloud projects add-iam-policy-binding ${GCP_PROJECT} --member=serviceAccount:dns-admin@${GCP_PROJECT}.iam.gserviceaccount.com --role=roles/dns.admin

# Create a Kubernetes secret with the GCP Cloud DNS admin key:
#
kubectl create secret generic cert-manager-credentials --from-file=./gcp-dns-admin.json --namespace=istio-system

## Create a letsencrypt issuer for CloudDNS (replace email@example.com with a valid email address and 
## my-gcp-projectwith your project ID):
#
apiVersion: certmanager.k8s.io/v1alpha1
kind: Issuer
metadata:
  name: letsencrypt-prod
  namespace: istio-system
spec:
  acme:
    server: https://acme-v02.api.letsencrypt.org/directory
    email: sebastian@weave.works
    privateKeySecretRef:
      name: letsencrypt-prod
    dns01:
      providers:
      - name: cloud-dns
        clouddns:
          serviceAccountSecretRef:
            name: cert-manager-credentials
            key: gcp-dns-admin.json
          project: ${GCP_PROJECT}


## Save the above resource as letsencrypt-issuer.yaml and then apply it:
#
kubectl apply -f ./letsencrypt-issuer.yaml

## Create a wildcard certificate (replace example.com with your domain):
#
apiVersion: certmanager.k8s.io/v1alpha1
kind: Certificate
metadata:
  name: istio-gateway
  namespace: istio-system
spec:
  secretName: istio-ingressgateway-certs
  issuerRef:
    name: letsencrypt-prod
  commonName: "*.${DOMAIN}"
  acme:
    config:
    - dns01:
        provider: cloud-dns
      domains:
      - "*.${DOMAIN}"
      - "${DOMAIN}"

#Save the above resource as istio-gateway-cert.yaml and then apply it:
#
kubectl apply -f ./istio-gateway-cert.yaml

# In a couple of seconds cert-manager should fetch a wildcard certificate from letsencrypt.org:
kubectl -n istio-system describe certificate istio-gateway

# Events:
#   Type    Reason         Age    From          Message
#   ----    ------         ----   ----          -------
#   Normal  CertIssued     1m52s  cert-manager  Certificate issued successfully

# Recreate Istio ingress gateway pods:
#
kubectl -n istio-system get pods -l istio=ingressgateway

## NOTE!
#
# Istio gateway doesn't reload the certificates from the TLS secret on cert-manager renewal. Since the 
# GKE cluster is made out of preemptible VMs the gateway pods will be replaced once every 24h, if you're 
# not using preemptible nodes then you need to manually delete the gateway pods every two months before 
# the certificate expires.

## Find GKE Istio version
#
kubectl -n istio-system get deploy istio-pilot -oyaml | grep image:

## Install Prometheus in istio-system namespace (replace 1.0.6-gke.3 with your version):
#
# 1.1.13-gke.0
kubectl -n istio-system apply -f https://storage.googleapis.com/gke-release/istio/release/1.1.13-gke.0/patches/install-prometheus.yaml

#!#!# error: unable to read URL "https://storage.googleapis.com/gke-release/istio/release/1.1.13-gke.0/patches/install-prometheus.yaml", server reported 404 Not Found, status code=404

# ---- START SIDEBAR ----  Install Prometheus on Istio

## Get the Helm charst for your Istio version / 1.1.13-gke.0
#
# see: https://github.com/istio/istio/releases
#
wget -q https://github.com/istio/istio/releases/download/1.1.13/istio-1.1.13-osx.tar.gz

## Generate the YAML file with Prometheus disabled.
#
helm template --set prometheus.enabled=false --namespace istio-system install/kubernetes/helm/istio > off.yaml

## Optionally, choose the Prometheus installation options that you want to override.

## Generate the YAML file with Prometheus enabled. If you want to change any default 
## values, include the --set KEY=VALUE option in the Helm command.
#
helm template --set prometheus.enabled=true --namespace istio-system install/kubernetes/helm/istio > on.yaml

## Create a unified diff file.
#
diff -u off.yaml on.yaml > prometheus.patch

## Manually edit the unified diff file to resolve the differences and save only the 
## YAML that is applicable to Prometheus in a manifest file.
#
## Caution: You must delete all the lines that don't apply to Prometheus from the file 
## to avoid service disruption.
#
# I extracted the sections from the diff out of on.yaml and into install-prom.yaml

## Apply the file to enable Prometheus. Replace MANIFEST with the name of your YAML file.
#
kubectl apply -f install-prom.yaml

# ---- END SIDEBAR ----  Install Prometheus on Istio

## Add Flagger Helm repository:
#
helm repo add flagger https://flagger.app

## Install Flagger's Canary CRD:
#
kubectl apply -f https://raw.githubusercontent.com/weaveworks/flagger/master/artifacts/flagger/crd.yaml

## Deploy Flagger in the istio-system namespace with Slack notifications enabled:
#
#helm upgrade -i flagger flagger/flagger \
#--namespace=istio-system \
#--set crd.create=false \
#--set metricsServer=http://prometheus.istio-system:9090 \
#--set slack.url=https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK \
#--set slack.channel=general \
#--set slack.user=flagger

## OR without Slack notifications enabled:
helm upgrade -i flagger flagger/flagger \
--namespace=istio-system \
--set crd.create=false \
--set meshProvider=istio \
--set metricsServer=http://prometheus:9090

## Deploy Grafana in the istio-system namespace:
#
helm upgrade -i flagger-grafana flagger/grafana \
--namespace=istio-system \
--set url=http://prometheus.istio-system:9090 \
--set user=admin \
--set password=replace-me

## Expose Grafana through the public gateway by creating a virtual service (replace example.com with your domain):
#
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: grafana
  namespace: istio-system
spec:
  hosts:
  - "grafana.${DOMAIN}"
  gateways:
  - public-gateway.istio-system.svc.cluster.local
  http:
  - route:
    - destination:
        host: flagger-grafana

## Save the above resource as grafana-virtual-service.yaml and then apply it:
#
kubectl apply -f ./grafana-virtual-service.yaml

## Navigate to http://grafana.example.com in your browser and you should be 
## redirected to the HTTPS version.
#

### BOOTSTRAP Flagger Canary Deploy Demo

# Set FLAGGER_REPO env var (not necessary if done above)
#
export FLAGGER_REPO=https://raw.githubusercontent.com/weaveworks/flagger/master

## Create a test namespace with Istio sidecar injection enabled:
#
kubectl apply -f ${FLAGGER_REPO}/artifacts/namespaces/test.yaml

## Create a deployment and a horizontal pod autoscaler:
#
kubectl apply -f ${FLAGGER_REPO}/artifacts/canaries/deployment.yaml
kubectl apply -f ${FLAGGER_REPO}/artifacts/canaries/hpa.yaml

## Deploy the load testing service to generate traffic during the canary analysis:
#
kubectl -n test apply -f ${FLAGGER_REPO}/artifacts/loadtester/deployment.yaml
kubectl -n test apply -f ${FLAGGER_REPO}/artifacts/loadtester/service.yaml

## Create a canary custom resource (replace example.com with your own domain):
#
apiVersion: flagger.app/v1alpha3
kind: Canary
metadata:
  name: podinfo
  namespace: test
spec:
  # deployment reference
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: podinfo
  # the maximum time in seconds for the canary deployment
  # to make progress before it is rollback (default 600s)
  progressDeadlineSeconds: 60
  # HPA reference (optional)
  autoscalerRef:
    apiVersion: autoscaling/v2beta1
    kind: HorizontalPodAutoscaler
    name: podinfo
  service:
    # container port
    port: 9898
    # Istio gateways (optional)
    gateways:
    - public-gateway.istio-system.svc.cluster.local
    # Istio virtual service host names (optional)
    hosts:
    - app.${DOMAIN}
  canaryAnalysis:
    # schedule interval (default 60s)
    interval: 1m
    # max number of failed metric checks before rollback
    threshold: 5
    # max traffic percentage routed to canary
    # percentage (0-100)
    maxWeight: 50
    # canary increment step
    # percentage (0-100)
    stepWeight: 10
    metrics:
    - name: request-success-rate
      # minimum req success rate (non 5xx responses)
      # percentage (0-100)
      threshold: 99
      interval: 1m
    - name: request-duration
      # maximum req duration P99
      # milliseconds
      threshold: 500
      interval: 30s
    # testing (optional)
    webhooks:
      - name: acceptance-test
        type: pre-rollout
        url: http://flagger-loadtester.test/
        timeout: 30s
        metadata:
          type: bash
          cmd: "curl -sd 'test' http://podinfo-canary:9898/token | grep token"
      - name: load-test
        url: http://flagger-loadtester.test/
        timeout: 5s
        metadata:
          cmd: "hey -z 1m -q 10 -c 2 http://podinfo-canary.test:9898/"

## Save the above resource as podinfo-canary-gke.yaml and then apply it:
#
kubectl apply -f ./podinfo-canary-gke.yaml


#### Useful commands for demoing
#
## Trigger a canary release
#
# kubectl -n test set image deployment/podinfo podinfod=stefanprodan/podinfo:2.1.2
#
## Exec to the Flagger pod (for rollbacks)
#
# kubectl -n test get pod -l app=flagger-loadtester -o jsonpath="{.items[].metadata.name}"
#
# kubectl -n test exec -it flagger-loadtester-<PODID> sh
#
# kubectl -n test exec -it `kubectl -n test get pod -l app=flagger-loadtester -o jsonpath="{.items[].metadata.name}"` sh
#
## Generate HTTP 500 errors (for rollbacks)
#
# watch curl http://podinfo-canary:9898/status/500
#
## Generate response latency (for rollbacks)
#
# watch curl http://podinfo-canary:9898/delay/1
#
## Watch/get flagger pod logs
#
# kubectl -n istio-system log flagger-<PODID> | jq -r .msg
#
# kubectl -n istio-system log `kubectl -n istio-system get pod --selector app.kubernetes.io/instance=flagger -o jsonpath={.items[].metadata.name}` -f | jq -r .msg
#
## Watch/get canary resource status
#
# watch kubectl -n test get canaries
#
## Watch/get result events
#
# kubectl -n test get event --field-selector involvedObject.kind=Canary -o json | jq -r .items[].message
#
